{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b598b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.applications import resnet50\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcdfd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>distribute_40</td>\n",
       "      <td>distribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>distribute_41</td>\n",
       "      <td>distribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>distribute_42</td>\n",
       "      <td>distribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>distribute_43</td>\n",
       "      <td>distribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>distribute_44</td>\n",
       "      <td>distribute</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             id        Type\n",
       "0           0  distribute_40  distribute\n",
       "1           1  distribute_41  distribute\n",
       "2           2  distribute_42  distribute\n",
       "3           3  distribute_43  distribute\n",
       "4           4  distribute_44  distribute"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('Data/train_label.csv')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e6ac0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distribute    360\n",
       "matrix        360\n",
       "ineq          360\n",
       "integral      360\n",
       "series        360\n",
       "sqrt          360\n",
       "limit         360\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f415388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>distribute</th>\n",
       "      <th>ineq</th>\n",
       "      <th>integral</th>\n",
       "      <th>limit</th>\n",
       "      <th>matrix</th>\n",
       "      <th>series</th>\n",
       "      <th>sqrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distribute_40</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distribute_41</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distribute_42</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distribute_43</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distribute_44</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  distribute      ineq  integral     limit    matrix  \\\n",
       "0  distribute_40    0.142857  0.142857  0.142857  0.142857  0.142857   \n",
       "1  distribute_41    0.142857  0.142857  0.142857  0.142857  0.142857   \n",
       "2  distribute_42    0.142857  0.142857  0.142857  0.142857  0.142857   \n",
       "3  distribute_43    0.142857  0.142857  0.142857  0.142857  0.142857   \n",
       "4  distribute_44    0.142857  0.142857  0.142857  0.142857  0.142857   \n",
       "\n",
       "     series      sqrt  \n",
       "0  0.142857  0.142857  \n",
       "1  0.142857  0.142857  \n",
       "2  0.142857  0.142857  \n",
       "3  0.142857  0.142857  \n",
       "4  0.142857  0.142857  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv('Data/sample_submission.csv')\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d08cfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_images: 2520\n",
      "Types: Index(['distribute', 'ineq', 'integral', 'limit', 'matrix', 'series', 'sqrt'], dtype='object')\n",
      "num_classes: 7\n"
     ]
    }
   ],
   "source": [
    "num_train_images = 2520  # we choose 3300 images for this assignment. It works for a machine having 8Gb Ram. You can adjust it if your Ram is different. \n",
    "split_point = 2240 # split the data into training data [0:3000] and val data [3000:]\n",
    "print('num_train_images:', num_train_images)\n",
    "types = sample.columns[1:]\n",
    "print('Types:', types)\n",
    "num_classes = len(types)\n",
    "print('num_classes:', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5b3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "img_width = 224\n",
    "\n",
    "def get_image(filename):\n",
    "    ########################################################################\n",
    "    # TODO: Your code here...\n",
    "    ########################################################################\n",
    "    original = load_img(filename, target_size=(224,224))\n",
    "    numpy_image = img_to_array(original)\n",
    "    image_batch = np.expand_dims(numpy_image, axis=0)\n",
    "    return image_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56418cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAmWElEQVR4nO3deXxU9b3/8ddnJguBhBQIhJ2E7SqIBoiIRZT+3KEti1IQrldtr4DLw2LrfQhNKWipveJ271WkKEVRcWvpVaoUpAgWgSoBFEEg7GEJEHYISzIzn98fM8kdMIEwCzPD+Twfjzwy852Z73wmZ+adc873zPmKqmKMcS5XrAswxsSWhYAxDmchYIzDWQgY43AWAsY4nIWAMQ4XtRAQkdtEZIOIbBKRMdF6HmNMeCQaxwmIiBsoAm4GdgLLgbtU9duIP5kxJizRWhPoAWxS1S2qWg68C/SP0nMZY8KQFKV+WwA7gq7vBK6p6c5ZWVmak5MTpVKMMQArVqzYr6qNz26PVghINW1nbHeIyAhgBEDr1q0pLCyMUinGGAAR2V5de7Q2B3YCrYKutwR2B99BVV9R1XxVzW/c+DvhZIy5SKIVAsuBDiKSKyIpwFBgdpSeyxgThqhsDqiqR0QeBuYBbmC6qq6NxnMZY8ITrX0CqOocYE60+jfGRIYdMWiMw1kIGONwFgLGOJyFgDEOZyFgjMNZCBjjcBYCxjichYAxDmchYIzDWQgY43AWAsY4nIWAMQ5nIWCMw1kIGONwFgLGOFzIISAirURkoYisE5G1IvLzQPsEEdklIl8FfvpGrlxjTKSFc1IRD/BLVV0pIhnAChGZH7jtBVV9NvzyjDHRFnIIqGoJUBK4fExE1uE/1bgxJoFEZJ+AiOQAXYEvAk0Pi8hqEZkuIg0i8RzGmOgIOwREJB2YBYxW1aPAFKAdkId/TeG5Gh43QkQKRaSwtLQ03DKMMSEKKwREJBl/AMxU1b8AqOpeVfWqqg94Ff+UZN9h8w4YEx/CGR0Q4I/AOlV9Pqi9WdDdBgJrQi/PGBNt4YwO9ALuBr4Rka8Cbb8C7hKRPPzTjm0DRobxHMaYKAtndOBzqp9z0OYaMCaB2BGDxjichYAxDmchYIzDWQgY43AWAsY4nIWAMQ5nIWCMw1kIGONwFgLGOJyFgDEOZyFgjMNZCBjjcBYCxjichYAxDmchYIzDWQgY43DhnFkIEdkGHAO8gEdV80WkIfAekIP/zEI/UdVD4ZVpjImWSKwJ/EBV81Q1P3B9DLBAVTsACwLXjTFxKhqbA/2BGYHLM4ABUXgOY0yEhBsCCnwiIitEZESgLTswO1HlLEVNqnugzTtgTHwIa58A0EtVd4tIE2C+iKyv7QNV9RXgFYD8/HwNsw5jTIjCWhNQ1d2B3/uA/8U/0cjeyrkHAr/3hVukMSZ6wpl8pF5gNmJEpB5wC/6JRmYD9wTudg/wYbhFGmOiJ5zNgWzgf/0TEZEEvK2qc0VkOfC+iPwMKAYGh1+mMSZawpl8ZAtwVTXtB4AbwynKGHPx2BGDxjichYAxDmchYIzDWQgY43AWAsY4nIWAMQ5nIWCMw1kIGONwFgLGOJyFgDEOZyFgjMNZCBjjcBYCxjichYAxDmchYIzDhXw+ARH5F/zzC1RqC/wG+B5wP1B59tBfqeqcUJ/HGBNd4ZxUZAOQByAibmAX/vMM3ge8oKrPRqJAY0x0RWpz4EZgs6puj1B/xpiLJFIhMBR4J+j6wyKyWkSmi0iDCD2HMSYKwg4BEUkBfgz8KdA0BWiHf1OhBHiuhsfZ5CPGxIFIrAncDqxU1b0AqrpXVb2q6gNexT8XwXeo6iuqmq+q+Y0bN45AGcZEnqqe8XMpikQI3EXQpkDlxCMBA/HPRWBMQjp9+jSlpaUcPXoUn88X63KiIqwQEJG6wM3AX4KaJ4nINyKyGvgB8Gg4z2FMLM2ZM4dBgwbx61//muLiYnw+3yW3RhDWXISqegJodFbb3WFVZEwc+fzzz/n6669ZsmQJR44cYfz48eTm5hKYdOeSYEcMGnMO99xzDz179iQ5OZk333yTl19+mQMHDlxSawMWAsacw1VXXcW4cePo1q0bbrebDz74gFWrVlFRUXHJBIGFgDHncd111/Hggw/SsmVLtmzZwn//93+zceNGvF7vJREEl1wIxHooR1U5ceIEu3fv5uDBg3i93pjVYiLD5XJx44030rNnT+rVq8ecOXMoKChg+/btMX+/RcIlEQInT55k27ZtFBcXc/r06ZjWUl5ezuLFi/n1r3/Nm2++ycGDB2Naj4mMFi1aMGbMGH7wgx9Qt25dPvzwQ/785z9z8uTJWJcWtksiBL766isef/xxfvvb37Jx48aY7rktKytj4cKFvPbaa3zyySfs2LEjZrWYyMrLy2PChAl069aNpKQkpkyZwsaNG21NINaOHTvG3Llzef/995k3bx7ffvttTOsJXj1M9DeH+a7u3bszePBgsrKy2L59O5MnT+bIkSMJvawTPgS++uorFixYAEBycjJJSWEd+hAWVcXr9VJRUVF1PZHfHKZ6/fv357rrriM1NZVp06bx17/+NaH3/SR8CGzatIkVK1aQlJREp06dyMnJOef9VZX9+/eze/fuiA/zqCrbt29n/fr1ALjdblyuhP8Tm7O0adOGkSNHkpOTg8vlYuLEiRw6dChhA/+SeIeKCGlpafTu3ZvOnTufc2EcPHiQ119/nWeeeYbi4uKILjiPx8Pq1atZunQpmZmZXHbZZdiXoy49qsqNN95Iv379SE1NZfPmzfzzn/+0EIgVEUFEcLlc1KtXj9TU1HPef9u2bbzyyitMnjyZefPmcerUqYgtPJ/Px9GjRzly5AgtW7bk2muvpVmzZud/oIkbF7IJN2TIEOrVqwfAu+++m7CbBAkfAmerDIXz3cfj8TB9+nT27NkTsecuLS1l48aNAGRkZNCgQQPcbnfE+jfRsX///qrvCNRmiLnyPda1a1fatm0LwBdffMHevXsTcm3gkguB88nNzeW2224jNTWVFStW8NFHH0VsbWD9+vXMnz+fevXq0alTJ1q3bh2Bis+kqlRUVDB37lzGjRvH448/zsSJE1m4cGHVDklTe7t27WL69OkUFBRUDfnVdog5OTmZfv36kZSURGlpKZ9++mlChkDsdqXHSMOGDRk0aBB///vfWbduHS+//DKDBw+madOmYfe9f/9+tmzZQvPmzenWrRutWrUKqR+Px4PH4+HAgQN88MEHrFu37ozbvV4vy5cvZ/Xq1VRUVFC3bl169OjBQw89xIABA2I6QlJbe/bsYc6cOaxcubLG+6Snp3PTTTdx7bXXIiJnfMBEhJSUFNxud8jHhZSWlvLOO+/w0ksvsXfvXho0aEB5efkF9XHXXXfx7LPPcvz4cf70pz/Rr18/GjZsmFDfMoz/d0sU5OXlccMNN7Bp0yY2bNhAWVlZRPpVVXw+HykpKWRkZJx3/0R1vF4vf/nLX/j44485cuQIhYWF7Nq1q9r7fu973yM9PZ0DBw6wZMkS0tLS6Nu3b0KEwJdffsmzzz77nYALlpqayoIFC+jQoUO1H6q+ffty5513kpKSEtKHbv369UybNo3du3eTn5/P0KFDad++Papa6/7atWtH7969mTt3LosXL+att97ikUceueBaYum87xYRmQ78ENinqlcE2hrin3MgB9gG/ERVDwVuGwv8DPACj6jqvKhUHobMzEzatWtXtb1eWFhI27ZtI5be4RwfUF5ezvPPP8+XX35Z1Uf79u2pX7/+GX2mp6czYMAAcnJyePXVV5k7dy67du1KmJ1TjRs3pkuXLiQnJ9e432T79u2sWLGCwsLCam8vLCzk4MGDjBw5kuTk5FovP1Xl+PHjrF69mo0bN5Kens4NN9zAbbfdRv369S/odbhcLgoKCli1ahUlJSVMnjyZHj160LNnz4RZG6jNv4zXgZeAN4LaxgALVPU/RWRM4PrjItIJ/5mHOwPNgb+LSEdVjbt3Zvfu3WndujVFRUU8//zz3HHHHXExpu92u/nhD39IZmYmIkLDhg259957yczMPCMEUlJSyM3NRVV56623Ylhx7akqBw4c4MCBA+Tm5jJhwgQOHz5c44fls88+Y9myZWfs6xAR3G43S5cupaioiGeeeYa6devy05/+tNZ1HDx4kHfeeYfJkyfj8/moU6cOzZo1IzMzs+o5LkSPHj34+c9/TkFBAZs3b+aLL76gZ8+eF9RHLJ03BFT1HyKSc1Zzf6BP4PIMYBHweKD9XVU9DWwVkU34TzS6LEL1Rkx+fj6XX345mzZtYuXKlXGzQyc5OZmRI0cyaNAgRITU1FRat25d7Sr+yZMnmTZtGsuW+f+8SUlJcRFkwSr/riLCzp07mT59OitWrGDYsGEMHTr0nI9t3749AwYMwOfznfHBFBEWLlzIww8/zI4dO3j77bfPGwKVdVQeyzFlyhTWr19PdnY2AwcOpHfv3iH/505KSqJfv36MGzeOioqKhPtSUagbj9mqWgKgqiUi0iTQ3gL4Z9D9dgba4k56ejoZGRlVw4VHjx6lUaNGMV+FExEaN2583oOMTp8+zauvvsrTTz/N3r17ady4MSNHjiQlJeUiVXphdu7cyeTJk3nzzTc5cOAAl19++XlDICsri6ysrGpva9asGY888gher5ejR4+es5/KAPD5fGzZsoU//OEPFBUV0aJFC0aOHMnw4cNp3rz5Be0LOFvw3z3egvh8Il1tdX/Bav/FxsO8Ax06dKjaeffKK6/EzdrA+VRUVDBlyhSeeuopdu/eTYMGDfjNb37DkCFD4m6n4MGDB5k6dSr3338/06dPp6SkhLp169KgQXhz0gRvu9fmg6uqbNu2jaeeeoqPP/4YgG7duvHTn/6UnJyckHbiBgs+E3Gsv85+oUINgb2VpxYP/N4XaN8JBI+LtQR2V9dBPMw7MGzYMNq0aYOI8OKLLyZECHg8Hl566SV+97vfsXfvXho1asQTTzzBv/3bv1XtR4gHhw8fZtKkSdx555088cQTfPrpp5SWllK/fn2GDRvGXXfdddFqqfxOx/jx45k1axanT5/m2muvpaCggGbNmuFyuWp1kNm5pKWl0aiR/5y7a9euTajvEoT6b2M2cA/wn4HfHwa1vy0iz+PfMdgB+DLcIqMlJyeHhg0bAiTE0V4+n4+ZM2fy29/+lkOHDpGdnc348eO5++67SU9Pv6i1nP2VaZfLxenTp/nTn/7EjBkz2LJlC4cOHeL48eN4PB7S0tL48Y9/zGOPPUanTp3CXhOoLZ/PR3FxMWPHjmX27Nl4PB569erFpEmT6Nq1a8RW3Rs0aEDfvn157bXXWLRoEUVFRVxzzTUR6TvaajNE+A7+nYBZIrITGI//w/++iPwMKAYGA6jqWhF5H/gW8AAPxePIQKXgb/nFewCA/w1dUFDAoUOHaNy4MRMnTmTYsGHUrVv3ojz/6dOnOXbsGB6Ph/379/Nf//VfzJvnHwGuPJjn+PHjVR/8pKQkGjRoQH5+PqNHj6Z79+5kZmZGfJOlpv/gqsru3bt57LHH+Oijj/D5fFx33XVMmjSJvLy8iNaRnp7Oj370I15//XVKS0v55ptvLp0QUNWa1tturOH+vwN+F05RF1ODBg2q3sCFhYVcc801cbNKfbbKN7WI0KZNG26++WZKS0tp06ZNyH36fD7279/Pnj17ajxarvLvMW/ePKZPn86RI0fw+XyUlZV9Z/vX5XLRrFkzWrVqxdVXX82oUaNo3rw59erVIykpKSp/25oOlz5x4gTjxo1j9uzZAFELAPC/7nbt2tGyZUuKi4tZsmQJgwcP5nvf+15Enyca4msvUgwMHz6czz77jMOHDzNhwgT+9re/xbqkGokIOTk5bN26lVWrVtG1a1c6duzIqFGj6NOnz3nPpVCd4uLiqtd9vqGtiooKTp8+XbXWlJycTO/evWnatCkejwdVpVGjRgwfPpz8/Hzcbjd16tSJWqjm5uZSVFTEpk2bePXVVxkxYsQZQ4HvvfceM2fORFW55ppreOaZZ8jLy4va3vvs7Gzy8vIoLi5mwYIFlJaWWggkgoEDB/LYY49x+PBhCgsLwxomqhTuTqaauFwunnvuOe69916OHj3KoUOHWL58OWvWrOHqq69m4sSJ9OrV64L63LdvH6tXr2bfvn3nvzPQtWtX+vTpg9vtJicnh6FDh1Z9nbZS8FGAwccJRNpzzz1H//79OXbsGJ9++in3339/1fLbsmULBQUFVFRU0KpVK5544omoBgD4j4Ls0aMHs2fPZteuXecduowXjg+B4FXUSI2xe73eqBy+KyL88Ic/5L333mPDhg2sW7eOqVOnUlZWxueff84vfvELXnjhBb7//e/X+sN3xRVX8Nxzz1FUVITH46nxfqpKSkoK1157LZdddllV35Uf9pqeJ5qbVrfffntV/7t37+bYsWNkZGSwY8cOHnzwQfbs2UNaWhrDhg2jT58+VQEQrZqCwz+hTi139tTLsfjp3r27Xiifz6c+n09fe+01rVu3rmZkZOj//M//XHA/qqqtW7dWQJs2baperzekPlRVZ8+erU2aNNGMjAx96KGHdPv27SH3VROfz6der1c9Ho8eO3ZMFy1apAMHDlRAk5OTtXPnzjpx4kQtLi6uum9t+qvtT+XfPfjxwdcvtm7duimgIqKTJk3S4uJivemmm1RE1OVyaa9evfTkyZMXpZZdu3bpLbfcooBedtllumXLlovyvLUFFGo1n7+YB4CGEQKnTp3SF198UVNTUzU7O1unTZt2wf2oqt5xxx0qIpqZmakrVqwI+U29detWvf/++xXQ7t2769y5c0PqpzYqP3w+n0/37NmjgwYNUvwHZml6eroOHTpUi4qKYv4hjbZVq1Zp/fr1q0K8Xbt26nK5VEQ0Ly9Pd+3addFe/5IlS9Ttdiugjz76qJaVlV2U562tmkIgsY5vDCIibN26lWXLluHz+ejSpQudO3cOqa/hw4fjcrkoKyvjnXfeCbmm5s2bc+2119KoUSP27t3Lhg0borZdWLnqKSI0adKEl19+mXvvvReA48ePM3v2bMaPH8/mzZuj8vzxonPnzjz99NOA/xwFmzdvRlXp3Lkzc+bMoVmzZudc/VdVPB4P//jHP/jFL37BW2+9FfIyO378eNUmQI8ePUhLSwupn4stYUMA/Hu2ly9fjtfrpXPnznTq1Cmk7bABAwbgcrnweDwsXLgw5HrcbjedOnUiPz+fPXv2sHz5cvbu3Rtyf7UlImRnZzN9+nTWrl1Lz549OXHiBLNnz+app56iuLg46jXEk5ycHObOnUvTpk3Pu/3v8/lYvHgxY8eO5YUXXmDmzJlVp4gLR7wOM1cnoUPg1KlTHD9+HPAfS56RkRFSP8E7dMJZeG63m7Zt29K5c2c8Hg+lpaUcOXIk5P4ulIjQqVMnpk6dyjXXXENZWRkrV65kzZo1F62Gi0lV+eabb3jggQcA/05et9vN9u3buf/++9mxY8c5d9CpKmVlZSxdupSlS5fSsGFD8vPzadWqVUj/TM7+pmOiSOgQOHsoLpyhuebNmwNw4MABNmzYEHJNbrf7OweihPKGOpfKN7bP58Pn832n/8svv5zRo0cD0RuujIXgD7QGvhD04IMPAlCnTh2GDBlC7969UVXmzZvHXXfdxezZszl06BBer7fqb1X5c+LECebMmcOMGTMA/6bFzTffHNJp4lWVY8eOVV1PpG8SJk6lUfboo49W7WeYOnVqyP24XK6qYcdDhw5x8ODBM75hFikVFRVVcxwEH+Sjqhw6dKjqv/+5/hMmKo/HQ1FREffddx9ffPEFLpeL2267jTfffJMpU6ZUHfW5dOlShgwZwujRo5k5cybz58/n4MGDlJWVsWbNGqZMmcKTTz7Jxo0badmyJX379q3ar3ShwVk5HZ6q0rBhw8Q61Xx1ewsv9k8oowOq/iG55s2bq8vl0nHjxoXUR6Vjx46py+VSQAcOHBhyPydOnNAZM2Zo+/btNSsrS59++mk9dOhQWLWdzev16po1a7RTp06ak5OjCxcurBoFOHLkiE6dOlUbNWqkIqK9e/fWxYsXR/T5Y+nkyZM6efJk7datW9UwYO/evXXz5s1Vf4M1a9boww8/rI0aNapapiKiWVlZOmjQIB0xYoReddVVVe1NmzbV8ePHa2lpacijKdu3b9fs7Oyq909lX/GES22IUNUfAi1atIhICPh8vqrhnSFDhoTVz6pVq7R///4K6A033KCLFi3SioqKsOoL5vF49NNPP1VAMzMz9fXXX68a71++fLl27NhRRUQ7dOig06ZN01OnTkXsuWOh8oPp8Xj097//vaampiqgderU0V69eum2bdvOGDL1+XxaVlamc+fO1a5du2pmZqZmZGRocnJy1TCqy+XStLQ0zc3N1QkTJuj+/fvDGk5dv359Vf/jx4+PuwBQrTkEEvaIQa/XS3l5edWReRqBVd7k5GS8Xi+HDx/m1KlT1KlT54L7EBGaN2/OFVdcwcKFC/nss8946qmnGD9+PFdffXXV/oJIbqe73W48Hg+7du3i448/pqioiPr16zNgwADuu+++hNo+rYnP5+Prr79m5syZlJeXk56ezujRo3nggQeqHQasW7cuN910Ex07dqS0tJQ9e/bwxhtvsHjxYgCaNm3Kv/7rv3L77bfTokWLsL/aHPz8iTI0WClhQ6CkpIRly5Zx9OhRsrOzadmyZdh99u7dm/nz57Ny5UrmzJnDoEGDQuqnSZMmDB8+nB07dvDnP/+ZTz75BLfbzbhx4+jevTvJyclh11pJRPD5fCxatIgnn3ySlStX4na7ad++Pdddd90lEwBFRUX86le/Yv369bjdbiZMmMADDzxAWlpajYHqdrvJzc0lNzcXn89Hnz59OHHiBKpKcnJyyKeFP5uqnnFa+ITbEVvd6sHF/gllc+DLL7/UXr16KaB33HGHbt68+YL7ONuCBQuqVhfHjBkTdn+rV6/Wu+++WzMzMxXQfv366bJly7S8vDysfoM3B+rUqaO33HKLtmnTRgFNSUnRLl266BtvvBF2/fFi9+7d+pOf/ERTUlI0JSVFr7/+et23b1/crHKfOHFCf/SjH6mIaEZGhs6aNStuagvGpbY5UF5eXnWMQMuWLavmhAtHp06dqs4tEIk9+ldccQW//OUvERH++te/MmfOHESEgoICunTpgsvlQtX/rbekpKQL2lSovM+pU6f45JNPcLlcNG/enKuvvprhw4dz5513hl1/vNi6dSuLFi2ivLycPn368Prrr5OVlRU3/3H37dvH3//+d1SVW2+9lZtvvjnWJV2QUCcfeQb4EVAObAbuU9XDgVOTrwMqB9r/qaqjolF4NGjQfoWdO3dSUlJCw4YNQ57hBuDKK6/k0UcfRVX56KOP+Oijj/B6vQwcOJDU1FR8Ph8ul4vc3Fx69OhRq28yigj169endevWHDlyBJfLRdOmTRk1ahSPPPLIJTckmJ6eTosWLRARxowZU3U5XsyaNQuPx0PdunW55ZZbSE9Pj6v6zifUyUfmA2NV1SMiTwNj8c87ALBZVfMiWWR13G531YczUjP/pqam0r59ezZu3Mj8+fP5/e9/z0033cTNN98c0s6eyrWKq666ikcffRSv18ucOXP429/+9p2Tl3Tp0oX58+fTpEmTGno7s98OHTrw9NNPs2LFClJSUrjyyisZPHhw1e2Xkg4dOjB27FhKSkq48sor42qm5yNHjvD222/j8Xjo2LEjffr0iXVJFyykyUdU9ZOgq/8ELvq6Z3Z2Nr179wb888FFQkZGBo8//jj//u//TmlpKS+++CIffvghS5YsCXnHY2UQ5OXl8dhjj1GnTh0KCwupqKio+rBWzl94IX1mZGQwdOjQ8567/1KQlpZWFXDx5oMPPmDt2rW4XC569OgR0ensLpZI7BP4Kf55CSvlisgq4Cjwa1VdXN2DRGQEMAK44Cm8VZWWLVvy4IMPMmjQoIiFQHJyctW3v06dOlV1Lr9w9+ZXvim6du1aNVVVcAh4PB5atGhRdeZjE98qN7f279/PSy+9RHl5OU2aNOGee+5JyNGYsEJARArwn1V4ZqCpBGitqgdEpDvwgYh0VtXvfDdTVV8BXgHIz8+/4I3YpKQk2rVrF7EAqJSdnc2wYcMoKSmhXr16/Md//EdIx5LXpG3bthHZiWliy+PxMGvWrKrvmdx66618//vfT7i1AAgjBETkHvw7DG8MDD+g/jkITwcurxCRzUBHoPppZUN/7kh2d4asrCwmTZrE8ePHqVOnDllZWQmZ7iZ6Kr+XMG3aNE6cOEFycjKjRo2Kq30VFyKkEBCR2/DvCLxBVU8EtTcGDqqqV0Ta4p98ZEtEKr1IXC4XDRs2tFVzc06zZs3i22+/xev10r17d6666qqE/WcR6uQjY4FUYH7gv3LlUOD1wJMi4gG8wChVPRil2o2Jmcsvv5wmTZrQtGlTCgoKSE5OrjrmI9GEOvnIH2u47yxgVrhFGRPPVJWBAweSlpZGq1atuP766xN2UwAS+LsDxsRSdnZ21TkdK0cLEnEtACwEjLlgZ3/YE/XDXykx92QYYyLGQsAYh7MQMMbhLASMcTgLAWMczkLAGIezEDDG4SwEjHE4CwFjHM5CwBiHsxAwxuEsBIxxOAsBYxzuvCEgItNFZJ+IrAlqmyAiu0Tkq8BP36DbxorIJhHZICK3RqtwY0xk1GZN4HXgtmraX1DVvMDPHAAR6QQMBToHHvOyiCTu2RaMcYDzhoCq/gOo7SnC+gPvquppVd0KbAJ6hFGfMSbKwtkn8LCIrA5sLlTO69wC2BF0n52Btu8QkREiUigihaWlpWGUYYwJR6ghMAVoB+Thn2vguUB7dadYqXZOAVV9RVXzVTU/kuf1N8ZcmJBCQFX3qqpXVX3Aq/zfKv9OoFXQXVsCu8Mr0RgTTSGFgIg0C7o6EKgcOZgNDBWRVBHJxT/vwJfhlWiMiaZQ5x3oIyJ5+Ff1twEjAVR1rYi8D3yLf3qyh1TVG5XKjTERIfEwl31+fr4WFkZ0pjJjzFlEZIWq5p/dbkcMGuNwFgLGOJyFgDEOZyFgjMNZCBjjcBYCxjichYAxDmchYIzDWQgY43AWAsY4nIWAMQ5nIWCMw1kIGONwFgLGOJyFgDEOF+q8A+8FzTmwTUS+CrTniMjJoNv+EMXajTERcN4zC+Gfd+Al4I3KBlUdUnlZRJ4DjgTdf7Oq5kWoPmNMlJ03BFT1HyKSU91tIiLAT4D/F+G6jDEXSbj7BHoDe1V1Y1BbroisEpHPRKR3mP0bY6KsNpsD53IX8E7Q9RKgtaoeEJHuwAci0llVj579QBEZAYwAaN26dZhlGGNCFfKagIgkAYOA9yrbAtOPHQhcXgFsBjpW93ibfMSY+BDO5sBNwHpV3VnZICKNKycgFZG2+Ocd2BJeicaYaKrNEOE7wDLgX0Rkp4j8LHDTUM7cFAC4HlgtIl8DfwZGqWptJzM1xsRAbUYH7qqh/d5q2mYBs8IvyxhzsdgRg8Y4nIWAMQ5nIWCMw1kIGONwFgLGOJyFgDEOZyFgjMNZCBjjcBYCxjichYAxDmchYIzDWQgY43AWAsY4nIWAMQ5nIWCMw9XmpCKtRGShiKwTkbUi8vNAe0MRmS8iGwO/GwQ9ZqyIbBKRDSJyazRfgDEmPLVZE/AAv1TVy4GewEMi0gkYAyxQ1Q7AgsB1ArcNBToDtwEvV55yzBgTf84bAqpaoqorA5ePAeuAFkB/YEbgbjOAAYHL/YF3Aycd3QpsAnpEuG5jTIRc0D6BwCQkXYEvgGxVLQF/UABNAndrAewIetjOQJsxJg7VOgREJB3/+QNHVzePQPBdq2nTavobISKFIlJYWlpa2zKMMRFWqxAQkWT8ATBTVf8SaN4rIs0CtzcD9gXadwKtgh7eEth9dp8274Ax8aE2owMC/BFYp6rPB900G7gncPke4MOg9qEikioiufjnHvgyciUbYyKpNtOQ9QLuBr6pnIIc+BXwn8D7gXkIioHBAKq6VkTeB77FP7LwkKp6I124MSYyajPvwOdUv50PcGMNj/kd8Lsw6jLGXCR2xKAxDmchYIzDWQgY43AWAsY4nIWAMQ5nIWCMw1kIGONwFgLGOJyFgDEOZyFgjMNZCBjjcBYCxjichYAxDmchYIzDWQgY43AWAsY4nIWAMQ5nIWCMw4nqd84GfvGLECkFyoD9sa4lDFkkdv2Q+K8h0euH6L6GNqr6nVN7x0UIAIhIoarmx7qOUCV6/ZD4ryHR64fYvAbbHDDG4SwEjHG4eAqBV2JdQJgSvX5I/NeQ6PVDDF5D3OwTMMbERjytCRhjYiDmISAit4nIBhHZJCJjYl1PbYnINhH5RkS+EpHCQFtDEZkvIhsDvxvEus5KIjJdRPaJyJqgthrrFZGxgWWyQURujU3VZ6rhNUwQkV2B5fCViPQNui2uXoOItBKRhSKyTkTWisjPA+2xXQ6qGrMfwA1sBtoCKcDXQKdY1nQBtW8Dss5qmwSMCVweAzwd6zqDarse6AasOV+9QKfAskgFcgPLyB2nr2EC8Fg194271wA0A7oFLmcARYE6Y7ocYr0m0APYpKpbVLUceBfoH+OawtEfmBG4PAMYELtSzqSq/wAOntVcU739gXdV9bSqbgU24V9WMVXDa6hJ3L0GVS1R1ZWBy8eAdUALYrwcYh0CLYAdQdd3BtoSgQKfiMgKERkRaMtW1RLwL3CgScyqq52a6k205fKwiKwObC5UrkrH9WsQkRygK/AFMV4OsQ6B6mY7TpThil6q2g24HXhIRK6PdUERlEjLZQrQDsgDSoDnAu1x+xpEJB2YBYxW1aPnums1bRF/DbEOgZ1Aq6DrLYHdMarlgqjq7sDvfcD/4l9N2ysizQACv/fFrsJaqanehFkuqrpXVb2q6gNe5f9Wl+PyNYhIMv4AmKmqfwk0x3Q5xDoElgMdRCRXRFKAocDsGNd0XiJST0QyKi8DtwBr8Nd+T+Bu9wAfxqbCWqup3tnAUBFJFZFcoAPwZQzqO6/KD0/AQPzLAeLwNYiIAH8E1qnq80E3xXY5xMEe377495JuBgpiXU8ta26Lf6/t18DayrqBRsACYGPgd8NY1xpU8zv4V5cr8P+H+dm56gUKAstkA3B7rOs/x2t4E/gGWB340DSL19cAXId/dX418FXgp2+sl4MdMWiMw8V6c8AYE2MWAsY4nIWAMQ5nIWCMw1kIGONwFgLGOJyFgDEOZyFgjMP9f+JIODmDuaQTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image = get_image('Data/train/distribute_41.png')\n",
    "plt.imshow(np.uint8(image))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "561cb5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 images loaded\n",
      "2000 images loaded\n"
     ]
    }
   ],
   "source": [
    "x_train = np.zeros((num_train_images, img_width, img_width, 3), dtype=np.uint8)\n",
    "y_train = np.zeros((num_train_images, num_classes), dtype=np.uint8)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in range(num_train_images):\n",
    "    x_train[i] = get_image('Data/train/%s.png' % labels['id'][i])\n",
    "    pos_arrays = (types == labels['Type'][i]).nonzero() # recall that types is the array of classes\n",
    "    pos = pos_arrays[0][0]\n",
    "    y_train[i][pos] = 1\n",
    "    count += 1\n",
    "    if(count % 1000 == 0): print(count, 'images loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eedbf45",
   "metadata": {},
   "source": [
    "### Resnet with MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc9d949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 197s 5s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import resnet50\n",
    "resnet_mse_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_mse_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_mse_features = resnet_mse_model.predict(resnet_mse_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_mse_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "835c4687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout, Activation, BatchNormalization\n",
    "\n",
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40d40014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 500)              2000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model_mse = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_mse.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4cece05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.compile(loss='MeanSquaredError', optimizer=\"sgd\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0894dc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 2s 49ms/step - loss: 0.1485 - accuracy: 0.1415 - val_loss: 0.1929 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.1457 - accuracy: 0.1647 - val_loss: 0.1797 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 0.1460 - accuracy: 0.1513 - val_loss: 0.1690 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.1463 - accuracy: 0.1536 - val_loss: 0.1603 - val_accuracy: 0.0036\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.1437 - accuracy: 0.1670 - val_loss: 0.1545 - val_accuracy: 0.0036\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.1428 - accuracy: 0.1768 - val_loss: 0.1496 - val_accuracy: 0.0071\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1432 - accuracy: 0.1688 - val_loss: 0.1458 - val_accuracy: 0.0107\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.1390 - accuracy: 0.2009 - val_loss: 0.1430 - val_accuracy: 0.0107\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1380 - accuracy: 0.2054 - val_loss: 0.1408 - val_accuracy: 0.0107\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.1354 - accuracy: 0.2219 - val_loss: 0.1391 - val_accuracy: 0.0107\n",
      "Epoch 11/30\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1345 - accuracy: 0.2196 - val_loss: 0.1378 - val_accuracy: 0.0071\n",
      "Epoch 12/30\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1338 - accuracy: 0.2219 - val_loss: 0.1370 - val_accuracy: 0.0071\n",
      "Epoch 13/30\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.1344 - accuracy: 0.2232 - val_loss: 0.1367 - val_accuracy: 0.0143\n",
      "Epoch 14/30\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1313 - accuracy: 0.2522 - val_loss: 0.1371 - val_accuracy: 0.0071\n",
      "Epoch 15/30\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1303 - accuracy: 0.2509 - val_loss: 0.1372 - val_accuracy: 0.0071\n",
      "Epoch 16/30\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.1303 - accuracy: 0.2594 - val_loss: 0.1375 - val_accuracy: 0.0071\n",
      "Epoch 16: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b51d48ba00>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_mse.fit(resnet_mse_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_mse_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef692a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.save('model_mse.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "508e3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_mse_model.predict(image_batch)\n",
    "        predictions = model_mse.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e96ea036",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_excel(r'./Data/test_2_label.xlsx',usecols=[1,1])\n",
    "y_test = y_test.to_numpy()\n",
    "y_test = np.squeeze(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c66a230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc2734",
   "metadata": {},
   "source": [
    "### Resnet with MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fb353af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 199s 5s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "resnet_mae_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_mae_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_mae_features = resnet_mae_model.predict(resnet_mae_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_mae_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b45e5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a135452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 500)              2000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mae = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_mae.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e7cdd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mae.compile(loss='MeanAbsoluteError', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a29a67f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 0.1599 - accuracy: 0.4875 - val_loss: 0.2856 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0974 - accuracy: 0.7031 - val_loss: 0.2856 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0805 - accuracy: 0.7464 - val_loss: 0.2855 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0730 - accuracy: 0.7746 - val_loss: 0.2853 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0649 - accuracy: 0.8013 - val_loss: 0.2844 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0582 - accuracy: 0.8237 - val_loss: 0.2824 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 0.0559 - accuracy: 0.8295 - val_loss: 0.2772 - val_accuracy: 0.0107\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0516 - accuracy: 0.8446 - val_loss: 0.2666 - val_accuracy: 0.0250\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.0490 - accuracy: 0.8513 - val_loss: 0.2276 - val_accuracy: 0.2286\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.0478 - accuracy: 0.8545 - val_loss: 0.0680 - val_accuracy: 0.7679\n",
      "Epoch 11/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0415 - accuracy: 0.8777 - val_loss: 0.0242 - val_accuracy: 0.9179\n",
      "Epoch 12/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0388 - accuracy: 0.8875 - val_loss: 0.0276 - val_accuracy: 0.9179\n",
      "Epoch 13/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0371 - accuracy: 0.8835 - val_loss: 0.0157 - val_accuracy: 0.9500\n",
      "Epoch 14/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0361 - accuracy: 0.8924 - val_loss: 0.0130 - val_accuracy: 0.9571\n",
      "Epoch 15/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0334 - accuracy: 0.9009 - val_loss: 0.0116 - val_accuracy: 0.9571\n",
      "Epoch 16/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0318 - accuracy: 0.9080 - val_loss: 0.0090 - val_accuracy: 0.9679\n",
      "Epoch 17/30\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.0295 - accuracy: 0.9143 - val_loss: 0.0067 - val_accuracy: 0.9786\n",
      "Epoch 18/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0309 - accuracy: 0.9129 - val_loss: 0.0059 - val_accuracy: 0.9857\n",
      "Epoch 19/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0300 - accuracy: 0.9094 - val_loss: 0.0051 - val_accuracy: 0.9857\n",
      "Epoch 20/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0287 - accuracy: 0.9183 - val_loss: 0.0045 - val_accuracy: 0.9857\n",
      "Epoch 21/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0275 - accuracy: 0.9183 - val_loss: 0.0036 - val_accuracy: 0.9893\n",
      "Epoch 22/30\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.0286 - accuracy: 0.9129 - val_loss: 0.0037 - val_accuracy: 0.9893\n",
      "Epoch 23/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0273 - accuracy: 0.9183 - val_loss: 0.0041 - val_accuracy: 0.9857\n",
      "Epoch 24/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0256 - accuracy: 0.9272 - val_loss: 0.0038 - val_accuracy: 0.9893\n",
      "Epoch 24: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b518c31dc0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_mae.fit(resnet_mae_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_mae_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fb5dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mae.save('model_mae.h5') # you may try load_model.ipynb to see how it's loaded (not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0a660b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction1 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_mae_model.predict(image_batch)\n",
    "        predictions = model_mae.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction1.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d6214dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7678571428571429\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction1 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df276e1b",
   "metadata": {},
   "source": [
    "### Resnet with Hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4cb4fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 210s 5s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "resnet_hinge_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_hinge_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_hinge_features = resnet_hinge_model.predict(resnet_hinge_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_hinge_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27dffd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cdb8ccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_4   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 500)              2000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_hinge = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_hinge.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ffa9a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hinge.compile(loss='Hinge', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "305bea91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 1.0194 - accuracy: 0.4661 - val_loss: 1.1386 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.9585 - accuracy: 0.6741 - val_loss: 1.1118 - val_accuracy: 0.0679\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.9379 - accuracy: 0.7500 - val_loss: 0.9989 - val_accuracy: 0.5321\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.9262 - accuracy: 0.7888 - val_loss: 0.9225 - val_accuracy: 0.7821\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.9174 - accuracy: 0.8174 - val_loss: 0.8923 - val_accuracy: 0.8929\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.9128 - accuracy: 0.8353 - val_loss: 0.8864 - val_accuracy: 0.9071\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.9083 - accuracy: 0.8464 - val_loss: 0.8762 - val_accuracy: 0.9429\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.9023 - accuracy: 0.8692 - val_loss: 0.8751 - val_accuracy: 0.9500\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.9006 - accuracy: 0.8754 - val_loss: 0.8729 - val_accuracy: 0.9536\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.8995 - accuracy: 0.8732 - val_loss: 0.8672 - val_accuracy: 0.9679\n",
      "Epoch 11/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.8943 - accuracy: 0.8933 - val_loss: 0.8650 - val_accuracy: 0.9750\n",
      "Epoch 12/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.8948 - accuracy: 0.8933 - val_loss: 0.8631 - val_accuracy: 0.9857\n",
      "Epoch 13/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.8914 - accuracy: 0.8987 - val_loss: 0.8634 - val_accuracy: 0.9821\n",
      "Epoch 14/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.8915 - accuracy: 0.9000 - val_loss: 0.8632 - val_accuracy: 0.9821\n",
      "Epoch 15/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.8891 - accuracy: 0.9058 - val_loss: 0.8612 - val_accuracy: 0.9893\n",
      "Epoch 16/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.8876 - accuracy: 0.9112 - val_loss: 0.8606 - val_accuracy: 0.9893\n",
      "Epoch 17/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.8889 - accuracy: 0.9080 - val_loss: 0.8615 - val_accuracy: 0.9893\n",
      "Epoch 18/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.8871 - accuracy: 0.9121 - val_loss: 0.8617 - val_accuracy: 0.9893\n",
      "Epoch 19/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.8846 - accuracy: 0.9219 - val_loss: 0.8611 - val_accuracy: 0.9893\n",
      "Epoch 19: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b51b2e1070>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_hinge.fit(resnet_hinge_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_hinge_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9608db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hinge.save('model_hinge.h5') # you may try load_model.ipynb to see how it's loaded (not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e12a970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction6 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_hinge_model.predict(image_batch)\n",
    "        predictions = model_hinge.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction6.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "daec8d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction6 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c231c",
   "metadata": {},
   "source": [
    "### Resnet with SquaredHinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e60b6389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 208s 5s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import resnet50\n",
    "resnet_sh_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_sh_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_sh_features = resnet_sh_model.predict(resnet_sh_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_sh_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d386662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "440da0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_5   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 500)              2000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_sh = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_sh.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2f1f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sh.compile(loss='SquaredHinge', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "423a0338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 1.1149 - accuracy: 0.4897 - val_loss: 1.2486 - val_accuracy: 0.2500\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.0189 - accuracy: 0.7004 - val_loss: 0.9309 - val_accuracy: 0.8679\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 0.9825 - accuracy: 0.7723 - val_loss: 0.9201 - val_accuracy: 0.8821\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 0.9655 - accuracy: 0.8071 - val_loss: 0.8980 - val_accuracy: 0.9214\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 0.9550 - accuracy: 0.8286 - val_loss: 0.8860 - val_accuracy: 0.9464\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.9435 - accuracy: 0.8540 - val_loss: 0.8900 - val_accuracy: 0.9429\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.9384 - accuracy: 0.8629 - val_loss: 0.8799 - val_accuracy: 0.9607\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 1s 51ms/step - loss: 0.9317 - accuracy: 0.8754 - val_loss: 0.8724 - val_accuracy: 0.9786\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.9287 - accuracy: 0.8754 - val_loss: 0.8678 - val_accuracy: 0.9857\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.9244 - accuracy: 0.8844 - val_loss: 0.8649 - val_accuracy: 0.9893\n",
      "Epoch 11/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.9217 - accuracy: 0.8888 - val_loss: 0.8639 - val_accuracy: 0.9893\n",
      "Epoch 12/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.9214 - accuracy: 0.8853 - val_loss: 0.8640 - val_accuracy: 0.9893\n",
      "Epoch 13/30\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.9174 - accuracy: 0.8978 - val_loss: 0.8646 - val_accuracy: 0.9893\n",
      "Epoch 14/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.9117 - accuracy: 0.9080 - val_loss: 0.8640 - val_accuracy: 0.9893\n",
      "Epoch 14: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b52587bc70>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_sh.fit(resnet_sh_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_sh_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "17cab451",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sh.save('model_sh.h5') # you may try load_model.ipynb to see how it's loaded (not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "96e1e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction6 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_sh_model.predict(image_batch)\n",
    "        predictions = model_sh.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction6.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b36a48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7571428571428571\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction6 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43982683",
   "metadata": {},
   "source": [
    "### Resnet Squared Hinge with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "158db54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 199s 5s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import resnet50\n",
    "resnet_shsgd_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_shsgd_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_shsgd_features = resnet_shsgd_model.predict(resnet_shsgd_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_shsgd_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "700c65de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b686938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_6   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 500)              2000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_shsgd = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_shsgd.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "71a17ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_shsgd.compile(loss='SquaredHinge', optimizer=\"sgd\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c9822b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 1.2427 - accuracy: 0.1781 - val_loss: 1.2234 - val_accuracy: 0.1214\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 1.2419 - accuracy: 0.1741 - val_loss: 1.2292 - val_accuracy: 0.0929\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 59ms/step - loss: 1.2371 - accuracy: 0.1830 - val_loss: 1.2340 - val_accuracy: 0.0536\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 1.2331 - accuracy: 0.2022 - val_loss: 1.2383 - val_accuracy: 0.0429\n",
      "Epoch 4: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b5276e4490>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_shsgd.fit(resnet_shsgd_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_shsgd_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a26d78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_shsgd.save('model_shsgd.h5') # you may try load_model.ipynb to see how it's loaded (not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c31bdd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction6 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_shsgd_model.predict(image_batch)\n",
    "        predictions = model_shsgd.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction6.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "424fa0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.24642857142857144\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction6 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4dcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
