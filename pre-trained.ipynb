{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c60563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.applications import resnet50\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c8328",
   "metadata": {},
   "source": [
    "# We first work with resnet_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddfaed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>distribute_40</td>\n",
       "      <td>distribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>distribute_41</td>\n",
       "      <td>distribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>distribute_42</td>\n",
       "      <td>distribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>distribute_43</td>\n",
       "      <td>distribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>distribute_44</td>\n",
       "      <td>distribute</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             id        Type\n",
       "0           0  distribute_40  distribute\n",
       "1           1  distribute_41  distribute\n",
       "2           2  distribute_42  distribute\n",
       "3           3  distribute_43  distribute\n",
       "4           4  distribute_44  distribute"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('Data/train_label.csv')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71b999bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "limit         360\n",
       "ineq          360\n",
       "integral      360\n",
       "series        360\n",
       "matrix        360\n",
       "distribute    360\n",
       "sqrt          360\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6186fe7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>distribute</th>\n",
       "      <th>ineq</th>\n",
       "      <th>integral</th>\n",
       "      <th>limit</th>\n",
       "      <th>matrix</th>\n",
       "      <th>series</th>\n",
       "      <th>sqrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distribute_40</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distribute_41</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distribute_42</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distribute_43</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distribute_44</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  distribute      ineq  integral     limit    matrix  \\\n",
       "0  distribute_40    0.142857  0.142857  0.142857  0.142857  0.142857   \n",
       "1  distribute_41    0.142857  0.142857  0.142857  0.142857  0.142857   \n",
       "2  distribute_42    0.142857  0.142857  0.142857  0.142857  0.142857   \n",
       "3  distribute_43    0.142857  0.142857  0.142857  0.142857  0.142857   \n",
       "4  distribute_44    0.142857  0.142857  0.142857  0.142857  0.142857   \n",
       "\n",
       "     series      sqrt  \n",
       "0  0.142857  0.142857  \n",
       "1  0.142857  0.142857  \n",
       "2  0.142857  0.142857  \n",
       "3  0.142857  0.142857  \n",
       "4  0.142857  0.142857  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv('Data/sample_submission.csv')\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24dccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_images: 2520\n",
      "Types: Index(['distribute', 'ineq', 'integral', 'limit', 'matrix', 'series', 'sqrt'], dtype='object')\n",
      "num_classes: 7\n"
     ]
    }
   ],
   "source": [
    "# num_train_images = labels.shape[0]\n",
    "num_train_images = 2520  # we choose 3300 images for this assignment. It works for a machine having 8Gb Ram. You can adjust it if your Ram is different. \n",
    "split_point = 2240 # split the data into training data [0:3000] and val data [3000:]\n",
    "print('num_train_images:', num_train_images)\n",
    "types = sample.columns[1:]\n",
    "print('Types:', types)\n",
    "num_classes = len(types)\n",
    "print('num_classes:', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9042bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "img_width = 224\n",
    "\n",
    "def get_image(filename):\n",
    "    ########################################################################\n",
    "    # TODO: Your code here...\n",
    "    ########################################################################\n",
    "    original = load_img(filename, target_size=(224,224))\n",
    "    numpy_image = img_to_array(original)\n",
    "    image_batch = np.expand_dims(numpy_image, axis=0)\n",
    "    return image_batch[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2e137e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image = get_image('Data/train/distribute_41.png')\n",
    "plt.imshow(np.uint8(image))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7954af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 images loaded\n",
      "2000 images loaded\n"
     ]
    }
   ],
   "source": [
    "x_train = np.zeros((num_train_images, img_width, img_width, 3), dtype=np.uint8)\n",
    "y_train = np.zeros((num_train_images, num_classes), dtype=np.uint8)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in range(num_train_images):\n",
    "    x_train[i] = get_image('Data/train/%s.png' % labels['id'][i])\n",
    "    pos_arrays = (types == labels['Type'][i]).nonzero() # recall that types is the array of classes\n",
    "    pos = pos_arrays[0][0]\n",
    "    y_train[i][pos] = 1\n",
    "    count += 1\n",
    "    if(count % 1000 == 0): print(count, 'images loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5266be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import resnet50\n",
    "\n",
    "x_train = resnet50.preprocess_input(x_train) # preprocess the images properly (e.g., minus the mean image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9d7fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet50 = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "# include_top=False --> do not load the last FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c2ed287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 204s 5s/step\n"
     ]
    }
   ],
   "source": [
    "#x_train_mini=np.expand_dims(x_train_test, axis=0)\n",
    "#features = model_resnet50.predict(x_train_test, batch_size=1, verbose=1)\n",
    "features = model_resnet50.predict(x_train, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8901b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "# Using 7 x 7 x 2048 features as input to an FC layer leads to many parameters, and may overfit for small dataset\n",
    "# Our solution is to use global pooling: pooling over every 7 x 7 images, to obtain 2048 features only\n",
    "# -> ref: https://arxiv.org/pdf/1312.4400.pdf Sec 3.2\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout, Activation, BatchNormalization\n",
    "\n",
    "# define the network\n",
    "# NOTE:\n",
    "# here we use \"output = Layer(configs) (input)\" + model(inputs, outputs)\n",
    "# you may also use model.add(layer) when building your own sequential model\n",
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9240ad03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 500)              2000      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0df2f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign loss function (over the output), optimizer algorithm, etc.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f18c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 2s 55ms/step - loss: 1.4636 - accuracy: 0.5031 - val_loss: 1.7556 - val_accuracy: 0.3143\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.7279 - accuracy: 0.7464 - val_loss: 0.2385 - val_accuracy: 0.9071\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.5957 - accuracy: 0.7821 - val_loss: 0.0965 - val_accuracy: 0.9571\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.5113 - accuracy: 0.8138 - val_loss: 0.0525 - val_accuracy: 0.9893\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.4548 - accuracy: 0.8411 - val_loss: 0.0414 - val_accuracy: 0.9893\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.4152 - accuracy: 0.8562 - val_loss: 0.0423 - val_accuracy: 0.9929\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.3619 - accuracy: 0.8737 - val_loss: 0.0391 - val_accuracy: 0.9929\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.3562 - accuracy: 0.8674 - val_loss: 0.0472 - val_accuracy: 0.9893\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.3237 - accuracy: 0.8857 - val_loss: 0.0489 - val_accuracy: 0.9929\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 0.3258 - accuracy: 0.8830 - val_loss: 0.0509 - val_accuracy: 0.9929\n",
      "Epoch 10: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14e2cb98a00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use first 3000 images for training, last 300 for validation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model.fit(features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b104ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to disk\n",
    "model.save('model1.h5') # you may try load_model.ipynb to see how it's loaded (not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c35ca131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_img = get_image('Data/Test_2/series_15.png')\n",
    "plt.imshow(np.uint8(test_img))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e27a7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'series'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "\n",
    "image_batch = np.expand_dims(test_img, axis=0)\n",
    "image_batch = np.copy(image_batch)\n",
    "\n",
    "image_batch = resnet50.preprocess_input(image_batch)\n",
    "feature_input = model_resnet50.predict(image_batch)\n",
    "predictions = model.predict(feature_input)\n",
    "pos = np.argmax(predictions)\n",
    "types[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b52bd",
   "metadata": {},
   "source": [
    "## This is where we test our model\n",
    "\n",
    "- \"Test\" data : from the same pool of data (total data: 2800, Test data: 280)\n",
    "- \"Test_2\" data : similar pool of data but different set (280)\n",
    "- \"Test_3\" data : samples collected from different people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92138eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name:  f_dist_1.png  Prediction:  distribute\n",
      "Image Name:  f_ineq.png  Prediction:  ineq\n",
      "Image Name:  f_int.png  Prediction:  distribute\n",
      "Image Name:  f_lim.png  Prediction:  limit\n",
      "Image Name:  f_matrix.png  Prediction:  matrix\n",
      "Image Name:  f_series.png  Prediction:  series\n",
      "Image Name:  f_sqrt.png  Prediction:  sqrt\n",
      "Image Name:  s_dist3.png  Prediction:  ineq\n",
      "Image Name:  s_ineq3.png  Prediction:  ineq\n",
      "Image Name:  s_int3.png  Prediction:  integral\n",
      "Image Name:  s_limit3.png  Prediction:  limit\n",
      "Image Name:  s_matrix3.png  Prediction:  matrix\n",
      "Image Name:  s_series3.png  Prediction:  series\n",
      "Image Name:  s_sqrt3.png  Prediction:  sqrt\n",
      "Image Name:  T_dist2.png  Prediction:  ineq\n",
      "Image Name:  t_ineq2.png  Prediction:  ineq\n",
      "Image Name:  t_int2.png  Prediction:  integral\n",
      "Image Name:  t_lim2.png  Prediction:  limit\n",
      "Image Name:  t_matrix2.png  Prediction:  integral\n",
      "Image Name:  t_series2.png  Prediction:  ineq\n",
      "Image Name:  t_sqrt2.png  Prediction:  sqrt\n"
     ]
    }
   ],
   "source": [
    "for files in os.listdir('./Data/Test_3/'):\n",
    "    \n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'Data/Test_3/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = model_resnet50.predict(image_batch)\n",
    "        predictions = model.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "\n",
    "        print('Image Name: ',files,' Prediction: ',types[pos])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f03ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = model_resnet50.predict(image_batch)\n",
    "        predictions = model.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction.append(pos)\n",
    "\n",
    "        #print('Image Name: ',files,' Prediction: ',types[pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8309c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_excel(r'./Data/test_2_label.xlsx',usecols=[1,1])\n",
    "y_test = y_test.to_numpy()\n",
    "y_test = np.squeeze(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53251f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7428571428571429\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adeddd5",
   "metadata": {},
   "source": [
    "### concatenating features from Resnet50 & Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce47d93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 195s 5s/step\n",
      "(2520, 7, 7, 2048)\n",
      "40/40 [==============================] - 470s 12s/step\n",
      "(2520, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import inception_v3\n",
    "from tensorflow.keras.applications import vgg16\n",
    "\n",
    "#getting the features of all three models\n",
    "\n",
    "#Resnet50 - last layer of features\n",
    "resnet50_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet50_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet50_features = resnet50_model.predict(resnet50_x_train, batch_size=64, verbose=1)\n",
    "print(resnet50_features.shape)\n",
    "\n",
    "#vgg16\n",
    "vgg16_x_train = vgg16.preprocess_input(x_train.copy())\n",
    "vgg16_model = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "vgg16_features = vgg16_model.predict(vgg16_x_train, batch_size=64, verbose=1)\n",
    "print(vgg16_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "591b0250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2520, 7, 7, 2560)\n"
     ]
    }
   ],
   "source": [
    "concat = np.concatenate([resnet50_features, vgg16_features],axis=3)\n",
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b02133f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 7)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 7, 7, 2560)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 2560)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2560)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 500)               1280500   \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 500)              2000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,286,007\n",
      "Trainable params: 1,285,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model_in = Input(shape=(7,7,2560)) \n",
    "mrg = GlobalAveragePooling2D()(new_model_in)\n",
    "mrg = Dropout(0.5)(mrg)\n",
    "mrg = Dense(500)(mrg) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "mrg = BatchNormalization()(mrg)\n",
    "mrg = Activation(\"relu\")(mrg)\n",
    "mrg = Dropout(0.5)(mrg) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(mrg)\n",
    "print(predictions.shape)\n",
    "#model_final=Model(inputs=[model1.input,model2.input],outputs=predictions)\n",
    "new_model = Model(inputs=new_model_in,outputs=predictions)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "485b6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e64984d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 2s 81ms/step - loss: 1.3138 - accuracy: 0.5469 - val_loss: 0.2204 - val_accuracy: 0.8857\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 51ms/step - loss: 0.6595 - accuracy: 0.7661 - val_loss: 0.0503 - val_accuracy: 0.9857\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.4815 - accuracy: 0.8304 - val_loss: 0.0189 - val_accuracy: 0.9964\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.4059 - accuracy: 0.8576 - val_loss: 0.0199 - val_accuracy: 0.9929\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.3847 - accuracy: 0.8723 - val_loss: 0.0244 - val_accuracy: 0.9929\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 0.3362 - accuracy: 0.8808 - val_loss: 0.0254 - val_accuracy: 0.9929\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14e2fa761c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "new_model.fit(concat[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(concat[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e0478e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction2 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        \n",
    "        image_batch1 = resnet50.preprocess_input(image_batch.copy())\n",
    "        image_batch2 = vgg16.preprocess_input(image_batch.copy())\n",
    "        feature_input1 = resnet50_model.predict(image_batch1)\n",
    "        feature_input2 = vgg16_model.predict(image_batch2)\n",
    "        concat2 = np.concatenate([feature_input1,feature_input2],axis=3)\n",
    "        predictions = new_model.predict(concat2)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction2.append(pos)\n",
    "\n",
    "        #print('Image Name: ',files,' Prediction: ',types[pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "101f99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.6892857142857143\n"
     ]
    }
   ],
   "source": [
    "y_test = pd.read_excel(r'./Data/test_2_label.xlsx',usecols=[1,1])\n",
    "y_test = y_test.to_numpy()\n",
    "y_test = np.squeeze(y_test)\n",
    "test_acc = (Test_prediction2 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980e7d7",
   "metadata": {},
   "source": [
    "### Here we use Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33e205ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 192s 5s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import xception\n",
    "\n",
    "#getting the features of all three models\n",
    "\n",
    "#Resnet50 - last layer of features\n",
    "xception_x_train = xception.preprocess_input(x_train.copy())\n",
    "xception_model = xception.Xception(weights='imagenet', include_top=False)\n",
    "xception_features = xception_model.predict(xception_x_train, batch_size=64, verbose=1)\n",
    "print(xception_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de82442f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2520, 7, 7, 2048)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xception_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c9b4b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f78ce39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 500)              2000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_x = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a966459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_x.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcca5e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 2s 49ms/step - loss: 1.4010 - accuracy: 0.5326 - val_loss: 1.8946 - val_accuracy: 0.1500\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.7665 - accuracy: 0.7312 - val_loss: 0.9696 - val_accuracy: 0.6821\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.5807 - accuracy: 0.7893 - val_loss: 0.6109 - val_accuracy: 0.8750\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.5352 - accuracy: 0.8112 - val_loss: 0.5362 - val_accuracy: 0.8821\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.4810 - accuracy: 0.8272 - val_loss: 0.4880 - val_accuracy: 0.8857\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.4278 - accuracy: 0.8536 - val_loss: 0.4463 - val_accuracy: 0.8536\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.4083 - accuracy: 0.8562 - val_loss: 0.4617 - val_accuracy: 0.8464\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.3908 - accuracy: 0.8598 - val_loss: 0.4769 - val_accuracy: 0.8429\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.3468 - accuracy: 0.8772 - val_loss: 0.3942 - val_accuracy: 0.8607\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.3386 - accuracy: 0.8795 - val_loss: 0.3424 - val_accuracy: 0.8714\n",
      "Epoch 11/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.3303 - accuracy: 0.8826 - val_loss: 0.3976 - val_accuracy: 0.8357\n",
      "Epoch 12/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 0.2909 - accuracy: 0.9013 - val_loss: 0.3421 - val_accuracy: 0.8607\n",
      "Epoch 13/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.2946 - accuracy: 0.9004 - val_loss: 0.2486 - val_accuracy: 0.9286\n",
      "Epoch 14/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 0.2876 - accuracy: 0.9013 - val_loss: 0.2432 - val_accuracy: 0.9179\n",
      "Epoch 15/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.2807 - accuracy: 0.8964 - val_loss: 0.2861 - val_accuracy: 0.9000\n",
      "Epoch 16/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.2764 - accuracy: 0.9040 - val_loss: 0.2119 - val_accuracy: 0.9250\n",
      "Epoch 17/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.2504 - accuracy: 0.9121 - val_loss: 0.2342 - val_accuracy: 0.9250\n",
      "Epoch 18/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.2731 - accuracy: 0.9076 - val_loss: 0.2825 - val_accuracy: 0.9036\n",
      "Epoch 19/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.2454 - accuracy: 0.9143 - val_loss: 0.2435 - val_accuracy: 0.9071\n",
      "Epoch 19: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14e39c3be20>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "model_x.fit(xception_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(xception_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5949358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_x.save('modelx.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4290d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction3 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = xception.preprocess_input(image_batch)\n",
    "        feature_input = xception_model.predict(image_batch)\n",
    "        predictions = model_x.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction3.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e362601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction3 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57657caf",
   "metadata": {},
   "source": [
    "### VGG16 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5596fa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 433s 11s/step\n",
      "(2520, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "#vgg16\n",
    "vgg16_x_train = vgg16.preprocess_input(x_train.copy())\n",
    "vgg16_model = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "vgg16_features = vgg16_model.predict(vgg16_x_train, batch_size=64, verbose=1)\n",
    "print(vgg16_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2113983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 512)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 512)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c9a47ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 7, 7, 512)]       0         \n",
      "                                                                 \n",
      " global_average_pooling2d_3   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 500)               256500    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 500)              2000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 262,007\n",
      "Trainable params: 261,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_v = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_v.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c83173fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b3721d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 1s 23ms/step - loss: 1.7974 - accuracy: 0.3482 - val_loss: 2.9429 - val_accuracy: 0.0964\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 1.1911 - accuracy: 0.5442 - val_loss: 0.3999 - val_accuracy: 0.8714\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 1.0248 - accuracy: 0.6237 - val_loss: 0.1720 - val_accuracy: 0.9536\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.9076 - accuracy: 0.6607 - val_loss: 0.1233 - val_accuracy: 0.9821\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.8716 - accuracy: 0.6871 - val_loss: 0.0623 - val_accuracy: 0.9929\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.7884 - accuracy: 0.7071 - val_loss: 0.0474 - val_accuracy: 0.9929\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.7667 - accuracy: 0.7205 - val_loss: 0.0422 - val_accuracy: 0.9929\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.7281 - accuracy: 0.7353 - val_loss: 0.0386 - val_accuracy: 0.9929\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.7141 - accuracy: 0.7473 - val_loss: 0.0430 - val_accuracy: 0.9929\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.6766 - accuracy: 0.7522 - val_loss: 0.0417 - val_accuracy: 0.9929\n",
      "Epoch 11/30\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.6410 - accuracy: 0.7696 - val_loss: 0.0373 - val_accuracy: 0.9929\n",
      "Epoch 12/30\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.6144 - accuracy: 0.7799 - val_loss: 0.0355 - val_accuracy: 0.9929\n",
      "Epoch 13/30\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.6101 - accuracy: 0.7804 - val_loss: 0.0298 - val_accuracy: 0.9929\n",
      "Epoch 14/30\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 0.5868 - accuracy: 0.7866 - val_loss: 0.0314 - val_accuracy: 0.9929\n",
      "Epoch 15/30\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.5864 - accuracy: 0.7915 - val_loss: 0.0335 - val_accuracy: 0.9929\n",
      "Epoch 16/30\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.5493 - accuracy: 0.8094 - val_loss: 0.0333 - val_accuracy: 0.9929\n",
      "Epoch 16: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14eefaecb20>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "model_v.fit(vgg16_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(vgg16_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "220912ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v.save('modelv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85a76a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction4 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = vgg16.preprocess_input(image_batch)\n",
    "        feature_input = vgg16_model.predict(image_batch)\n",
    "        predictions = model_v.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction4.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52c35e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.6535714285714286\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction4 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b99e7",
   "metadata": {},
   "source": [
    "### We will now use Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e1012cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 104s 3s/step\n",
      "(2520, 5, 5, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import inception_v3\n",
    "inceptionV3_x_train = inception_v3.preprocess_input(x_train.copy())\n",
    "inceptionV3_model = inception_v3.InceptionV3(weights='imagenet', include_top=False)\n",
    "inceptionV3_features = inceptionV3_model.predict(inceptionV3_x_train, batch_size=64, verbose=1)\n",
    "print(inceptionV3_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "283d6470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (5, 5, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0fb0fa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 5, 5, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_4   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_102 (Ba  (None, 500)              2000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_98 (Activation)  (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_i = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "811391c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_i.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "293ecc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.3626 - accuracy: 0.5205 - val_loss: 1.0535 - val_accuracy: 0.6679\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.7764 - accuracy: 0.7254 - val_loss: 0.3698 - val_accuracy: 0.8893\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6273 - accuracy: 0.7710 - val_loss: 0.1912 - val_accuracy: 0.9500\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5509 - accuracy: 0.8107 - val_loss: 0.1637 - val_accuracy: 0.9571\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5022 - accuracy: 0.8196 - val_loss: 0.1384 - val_accuracy: 0.9643\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4638 - accuracy: 0.8281 - val_loss: 0.1611 - val_accuracy: 0.9643\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4305 - accuracy: 0.8455 - val_loss: 0.0816 - val_accuracy: 0.9857\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3805 - accuracy: 0.8621 - val_loss: 0.1222 - val_accuracy: 0.9607\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.3638 - accuracy: 0.8741 - val_loss: 0.1200 - val_accuracy: 0.9607\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3416 - accuracy: 0.8763 - val_loss: 0.0934 - val_accuracy: 0.9714\n",
      "Epoch 10: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14e32d39be0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "model_i.fit(inceptionV3_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(inceptionV3_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7a705f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_i.save('modeli.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b2d206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction5 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = inception_v3.preprocess_input(image_batch)\n",
    "        feature_input = inceptionV3_model.predict(image_batch)\n",
    "        predictions = model_i.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction5.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3af23db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.6\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction5 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9022b48d",
   "metadata": {},
   "source": [
    "### Resnet with tanh activation funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec7bda5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 196s 5s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import resnet50\n",
    "resnet_tanh_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_tanh_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_tanh_features = resnet_tanh_model.predict(resnet_tanh_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_tanh_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc185e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout, Activation, BatchNormalization\n",
    "\n",
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"tanh\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "98588e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_5   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_103 (Ba  (None, 500)              2000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_99 (Activation)  (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "model_tanh = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_tanh.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a1c6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tanh.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "05f008f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 3s 55ms/step - loss: 1.2176 - accuracy: 0.5701 - val_loss: 0.5353 - val_accuracy: 0.7643\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.6521 - accuracy: 0.7741 - val_loss: 0.0368 - val_accuracy: 0.9857\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.4998 - accuracy: 0.8295 - val_loss: 0.0293 - val_accuracy: 0.9929\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.4512 - accuracy: 0.8402 - val_loss: 0.0355 - val_accuracy: 0.9964\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.3991 - accuracy: 0.8647 - val_loss: 0.0399 - val_accuracy: 0.9929\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.3496 - accuracy: 0.8759 - val_loss: 0.0365 - val_accuracy: 0.9929\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14efa021e50>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_tanh.fit(resnet_tanh_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_tanh_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b35afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tanh.save('model_tanh.h5') # you may try load_model.ipynb to see how it's loaded (not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a17e41ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction6 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_tanh_model.predict(image_batch)\n",
    "        predictions = model_tanh.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction6.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "763a4fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.675\n"
     ]
    }
   ],
   "source": [
    "y_test = pd.read_excel(r'./Data/test_2_label.xlsx',usecols=[1,1])\n",
    "y_test = y_test.to_numpy()\n",
    "y_test = np.squeeze(y_test)\n",
    "test_acc = (Test_prediction6 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a858b",
   "metadata": {},
   "source": [
    "### Resnet with RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e5e8b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 189s 5s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import resnet50\n",
    "resnet_relu_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_relu_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_relu_features = resnet_relu_model.predict(resnet_relu_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_relu_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c78e56e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "70d2d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_6   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_104 (Ba  (None, 500)              2000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_100 (Activation)  (None, 500)              0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_relu = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_relu.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f239478",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d3173767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 2s 45ms/step - loss: 1.3583 - accuracy: 0.5277 - val_loss: 1.4639 - val_accuracy: 0.5143\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.7015 - accuracy: 0.7473 - val_loss: 0.1959 - val_accuracy: 0.9464\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.5539 - accuracy: 0.8094 - val_loss: 0.0961 - val_accuracy: 0.9607\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.4642 - accuracy: 0.8384 - val_loss: 0.0702 - val_accuracy: 0.9750\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.4168 - accuracy: 0.8522 - val_loss: 0.0468 - val_accuracy: 0.9857\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.3980 - accuracy: 0.8594 - val_loss: 0.0325 - val_accuracy: 0.9929\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 0.3788 - accuracy: 0.8746 - val_loss: 0.0398 - val_accuracy: 0.9929\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3312 - accuracy: 0.8853 - val_loss: 0.0390 - val_accuracy: 0.9929\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3226 - accuracy: 0.8871 - val_loss: 0.0428 - val_accuracy: 0.9929\n",
      "Epoch 9: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1500313c430>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_relu.fit(resnet_relu_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_relu_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e0ffbfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu.save('model_soft.h5') # you may try load_model.ipynb to see how it's loaded (not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d9a1fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction9 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_relu_model.predict(image_batch)\n",
    "        predictions = model_relu.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction9.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "05219d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.6821428571428572\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction9 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab49f7",
   "metadata": {},
   "source": [
    "### Resnet with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d2abbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 182s 5s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import resnet50\n",
    "resnet_soft_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_soft_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_soft_features = resnet_soft_model.predict(resnet_soft_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_soft_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ca6ef4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"softmax\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b65856f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_7   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_105 (Ba  (None, 500)              2000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_101 (Activation)  (None, 500)              0         \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_soft = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_soft.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "13a3cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_soft.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "481e3a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 2s 53ms/step - loss: 1.9338 - accuracy: 0.4121 - val_loss: 1.9603 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.9139 - accuracy: 0.5884 - val_loss: 1.9857 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.8978 - accuracy: 0.6643 - val_loss: 2.0097 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.8822 - accuracy: 0.7272 - val_loss: 2.0332 - val_accuracy: 0.0000e+00\n",
      "Epoch 4: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x150089612b0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_soft.fit(resnet_soft_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_soft_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "99bcdbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_soft.save('model_soft.h5') # you may try load_model.ipynb to see how it's loaded (not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3d7dc343",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction6 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_soft_model.predict(image_batch)\n",
    "        predictions = model_soft.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction6.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3663ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.41785714285714287\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction6 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8d757",
   "metadata": {},
   "source": [
    "### Resnet with elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8bc30dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 182s 4s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import resnet50\n",
    "resnet_elu_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_elu_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_elu_features = resnet_elu_model.predict(resnet_elu_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_elu_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "57b60e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"elu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6ef81ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_8   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_106 (Ba  (None, 500)              2000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_102 (Activation)  (None, 500)              0         \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_elu = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_elu.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cfe8e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_elu.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e5267af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 2s 46ms/step - loss: 1.3365 - accuracy: 0.5603 - val_loss: 0.8229 - val_accuracy: 0.7250\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.6598 - accuracy: 0.7839 - val_loss: 0.0966 - val_accuracy: 0.9536\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.5395 - accuracy: 0.8129 - val_loss: 0.0178 - val_accuracy: 0.9964\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.4257 - accuracy: 0.8504 - val_loss: 0.0272 - val_accuracy: 0.9964\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.4209 - accuracy: 0.8549 - val_loss: 0.0268 - val_accuracy: 0.9929\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.3866 - accuracy: 0.8665 - val_loss: 0.0273 - val_accuracy: 0.9929\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x150aca65ac0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_elu.fit(resnet_elu_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_elu_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d1cf9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_elu.save('model_elu.h5') # you may try load_model.ipynb to see how it's loaded (not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8864f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction6 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_elu_model.predict(image_batch)\n",
    "        predictions = model_elu.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction6.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "131440a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.6642857142857143\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction6 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a58f57",
   "metadata": {},
   "source": [
    "### Resnet with exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "50bee9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 181s 4s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import resnet50\n",
    "resnet_expo_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_expo_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_expo_features = resnet_expo_model.predict(resnet_expo_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_expo_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8dac333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"exponential\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "afbe03e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_9   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_107 (Ba  (None, 500)              2000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_103 (Activation)  (None, 500)              0         \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_expo = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_expo.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0846c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_expo.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "813a0b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 3.4464 - accuracy: 0.4652 - val_loss: 2.3774 - val_accuracy: 0.7179\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 1.6544 - accuracy: 0.6683 - val_loss: 0.6338 - val_accuracy: 0.9071\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 1.2241 - accuracy: 0.7321 - val_loss: 0.2070 - val_accuracy: 0.9643\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 1.0096 - accuracy: 0.7661 - val_loss: 0.1569 - val_accuracy: 0.9679\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.9016 - accuracy: 0.7777 - val_loss: 0.1205 - val_accuracy: 0.9750\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.8269 - accuracy: 0.7969 - val_loss: 0.1039 - val_accuracy: 0.9821\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.7586 - accuracy: 0.8085 - val_loss: 0.0920 - val_accuracy: 0.9821\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.7239 - accuracy: 0.8107 - val_loss: 0.0704 - val_accuracy: 0.9929\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.6311 - accuracy: 0.8259 - val_loss: 0.0810 - val_accuracy: 0.9929\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 0.5608 - accuracy: 0.8446 - val_loss: 0.0884 - val_accuracy: 0.9929\n",
      "Epoch 11/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.5562 - accuracy: 0.8469 - val_loss: 0.0731 - val_accuracy: 0.9929\n",
      "Epoch 11: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1500338ca30>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_expo.fit(resnet_expo_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_expo_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f8176ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_expo.save('model_expo.h5') # you may try load_model.ipynb to see how it's loaded (not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fb6de73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction7 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_expo_model.predict(image_batch)\n",
    "        predictions = model_expo.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction7.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eb43a89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction7 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3440bfad",
   "metadata": {},
   "source": [
    "### Using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "da998eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 179s 4s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import resnet50\n",
    "resnet_sgd_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_sgd_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_sgd_features = resnet_sgd_model.predict(resnet_sgd_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_sgd_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "72df2261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f4270e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_24 (InputLayer)       [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_10  (None, 2048)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_108 (Ba  (None, 500)              2000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_104 (Activation)  (None, 500)              0         \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_sgd = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_sgd.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cc7fae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd.compile(loss='categorical_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bc39e3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 2s 81ms/step - loss: 2.2366 - accuracy: 0.2179 - val_loss: 3.3527 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 1.8708 - accuracy: 0.3259 - val_loss: 3.0075 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 1.6175 - accuracy: 0.4054 - val_loss: 2.6012 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 1.4398 - accuracy: 0.4665 - val_loss: 2.0642 - val_accuracy: 0.0857\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 1.4074 - accuracy: 0.4812 - val_loss: 1.8881 - val_accuracy: 0.1571\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 1.3162 - accuracy: 0.5147 - val_loss: 1.5696 - val_accuracy: 0.3714\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 1.2648 - accuracy: 0.5326 - val_loss: 1.3439 - val_accuracy: 0.4429\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 1.2142 - accuracy: 0.5549 - val_loss: 1.2265 - val_accuracy: 0.5179\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 1.1554 - accuracy: 0.5714 - val_loss: 1.1181 - val_accuracy: 0.5857\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 1.1290 - accuracy: 0.6045 - val_loss: 0.9979 - val_accuracy: 0.6500\n",
      "Epoch 11/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 1.1025 - accuracy: 0.6040 - val_loss: 0.9235 - val_accuracy: 0.6964\n",
      "Epoch 12/30\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 1.0891 - accuracy: 0.5978 - val_loss: 0.8897 - val_accuracy: 0.7250\n",
      "Epoch 13/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 1.0551 - accuracy: 0.6223 - val_loss: 0.8104 - val_accuracy: 0.7536\n",
      "Epoch 14/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.9973 - accuracy: 0.6317 - val_loss: 0.7621 - val_accuracy: 0.7821\n",
      "Epoch 15/30\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.0013 - accuracy: 0.6384 - val_loss: 0.7521 - val_accuracy: 0.7786\n",
      "Epoch 16/30\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.9801 - accuracy: 0.6438 - val_loss: 0.7089 - val_accuracy: 0.8071\n",
      "Epoch 17/30\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.9714 - accuracy: 0.6388 - val_loss: 0.7052 - val_accuracy: 0.8143\n",
      "Epoch 18/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.9307 - accuracy: 0.6540 - val_loss: 0.6482 - val_accuracy: 0.8429\n",
      "Epoch 19/30\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.9325 - accuracy: 0.6750 - val_loss: 0.5926 - val_accuracy: 0.8643\n",
      "Epoch 20/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.8835 - accuracy: 0.6670 - val_loss: 0.5669 - val_accuracy: 0.8750\n",
      "Epoch 21/30\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.8839 - accuracy: 0.6826 - val_loss: 0.5609 - val_accuracy: 0.8750\n",
      "Epoch 22/30\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.8991 - accuracy: 0.6737 - val_loss: 0.5336 - val_accuracy: 0.8750\n",
      "Epoch 23/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.8398 - accuracy: 0.6969 - val_loss: 0.5068 - val_accuracy: 0.8750\n",
      "Epoch 24/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.8739 - accuracy: 0.6812 - val_loss: 0.4960 - val_accuracy: 0.8786\n",
      "Epoch 25/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.8494 - accuracy: 0.6942 - val_loss: 0.4827 - val_accuracy: 0.8786\n",
      "Epoch 26/30\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.8540 - accuracy: 0.6839 - val_loss: 0.4705 - val_accuracy: 0.8786\n",
      "Epoch 27/30\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.8083 - accuracy: 0.7049 - val_loss: 0.4632 - val_accuracy: 0.8857\n",
      "Epoch 28/30\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.8185 - accuracy: 0.7000 - val_loss: 0.4222 - val_accuracy: 0.9107\n",
      "Epoch 29/30\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.8248 - accuracy: 0.7036 - val_loss: 0.4223 - val_accuracy: 0.9107\n",
      "Epoch 30/30\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.7848 - accuracy: 0.7129 - val_loss: 0.4122 - val_accuracy: 0.8964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1510c835d00>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_sgd.fit(resnet_sgd_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_sgd_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "acb84d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_sgd.save('model_sgd.h5') # you may try load_model.ipynb to see how it's loaded (not required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d86fe0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction8 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_sgd_model.predict(image_batch)\n",
    "        predictions = model_sgd.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction8.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "beddd1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.5821428571428572\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction8 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15852e",
   "metadata": {},
   "source": [
    "### Resnet with RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1150cf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 179s 4s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import resnet50\n",
    "resnet_rms_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_rms_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_rms_features = resnet_rms_model.predict(resnet_rms_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_rms_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e1da3b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "126956aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_26 (InputLayer)       [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_11  (None, 2048)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_109 (Ba  (None, 500)              2000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_105 (Activation)  (None, 500)              0         \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rms = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_rms.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7d25e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms.compile(loss='categorical_crossentropy', optimizer=\"RMSprop\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "240b21a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 2s 63ms/step - loss: 1.2116 - accuracy: 0.5741 - val_loss: 0.4746 - val_accuracy: 0.8214\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 0.7274 - accuracy: 0.7411 - val_loss: 0.2068 - val_accuracy: 0.9393\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 51ms/step - loss: 0.5612 - accuracy: 0.7933 - val_loss: 0.1177 - val_accuracy: 0.9643\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 51ms/step - loss: 0.4683 - accuracy: 0.8348 - val_loss: 0.0745 - val_accuracy: 0.9714\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 0.4278 - accuracy: 0.8487 - val_loss: 0.0338 - val_accuracy: 0.9929\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 0.3884 - accuracy: 0.8616 - val_loss: 0.0314 - val_accuracy: 0.9929\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 0.3776 - accuracy: 0.8567 - val_loss: 0.0412 - val_accuracy: 0.9929\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 1s 51ms/step - loss: 0.3424 - accuracy: 0.8795 - val_loss: 0.0359 - val_accuracy: 0.9929\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 0.3467 - accuracy: 0.8839 - val_loss: 0.0422 - val_accuracy: 0.9929\n",
      "Epoch 9: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x150af6c0ac0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_rms.fit(resnet_rms_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_rms_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3f769533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms.save('model_rms.h5') # you may try load_model.ipynb to see how it's loaded (not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c07fb2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction9 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_rms_model.predict(image_batch)\n",
    "        predictions = model_rms.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction9.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8be38b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.6892857142857143\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction9 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0745bb",
   "metadata": {},
   "source": [
    "### Resnet with Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "60b2574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 256s 6s/step\n",
      "(2520, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import resnet50\n",
    "resnet_del_x_train = resnet50.preprocess_input(x_train.copy())\n",
    "resnet_del_model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "resnet_del_features = resnet_del_model.predict(resnet_del_x_train, batch_size=64, verbose=1)\n",
    "print(resnet_del_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d0d4e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (7, 7, 2048)) # to take 7 x 7 x 2048 images\n",
    "x = GlobalAveragePooling2D()(inputs) # to convert to 2048 feagures\n",
    "print(x.shape)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "x = Dense(500)(x) # add a dense layer, but not adding activation so that we can add batch-norm first\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x) # add a dropout layer\n",
    "# Softmax layer to the output classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # arg1 is: units = dimensionality of the output space.\n",
    "print(predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c03dc7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_28 (InputLayer)       [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_12  (None, 2048)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 500)               1024500   \n",
      "                                                                 \n",
      " batch_normalization_110 (Ba  (None, 500)              2000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_106 (Activation)  (None, 500)              0         \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 7)                 3507      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,007\n",
      "Trainable params: 1,029,007\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_del = Model(inputs=inputs, outputs=predictions) # specify what is network input, and what is network output\n",
    "model_del.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fef200e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_del.compile(loss='categorical_crossentropy', optimizer=\"Adadelta\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8c678b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 2.5559 - accuracy: 0.1603 - val_loss: 3.9653 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 2.5143 - accuracy: 0.1683 - val_loss: 3.5570 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 2.5056 - accuracy: 0.1589 - val_loss: 3.3006 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.5337 - accuracy: 0.1491 - val_loss: 3.1109 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 2.5424 - accuracy: 0.1437 - val_loss: 2.9789 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 2.4788 - accuracy: 0.1750 - val_loss: 2.8859 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.4622 - accuracy: 0.1634 - val_loss: 2.8198 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 2.4663 - accuracy: 0.1705 - val_loss: 2.7726 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 2.4598 - accuracy: 0.1714 - val_loss: 2.7343 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.4833 - accuracy: 0.1817 - val_loss: 2.7082 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 2.4091 - accuracy: 0.1817 - val_loss: 2.6914 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 2.3868 - accuracy: 0.1897 - val_loss: 2.6741 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 2.3977 - accuracy: 0.1781 - val_loss: 2.6633 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/30\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 2.4295 - accuracy: 0.1786 - val_loss: 2.6499 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/30\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 2.4152 - accuracy: 0.1714 - val_loss: 2.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/30\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 2.3679 - accuracy: 0.2000 - val_loss: 2.6402 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.3372 - accuracy: 0.1942 - val_loss: 2.6346 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.3742 - accuracy: 0.1902 - val_loss: 2.6277 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.3385 - accuracy: 0.1888 - val_loss: 2.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.3427 - accuracy: 0.2018 - val_loss: 2.6174 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.3256 - accuracy: 0.2009 - val_loss: 2.6185 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.3888 - accuracy: 0.1830 - val_loss: 2.6171 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 2.2955 - accuracy: 0.2112 - val_loss: 2.6152 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 2.3409 - accuracy: 0.2049 - val_loss: 2.6099 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.2959 - accuracy: 0.2058 - val_loss: 2.6063 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.3089 - accuracy: 0.1924 - val_loss: 2.6073 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.2664 - accuracy: 0.2045 - val_loss: 2.6026 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 2.2459 - accuracy: 0.2214 - val_loss: 2.5984 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.2668 - accuracy: 0.2250 - val_loss: 2.5886 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 2.2388 - accuracy: 0.2344 - val_loss: 2.5852 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1512e027460>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1)\n",
    "# stop if loss does not improve for 3 iterations\n",
    "\n",
    "model_del.fit(resnet_del_features[:split_point], y_train[:split_point], batch_size=128, epochs=30, \n",
    "              validation_data=(resnet_del_features[split_point:], y_train[split_point:]), callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "282504e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_del.save('model_del.h5') # you may try load_model.ipynb to see how it's loaded (not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dce6512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_prediction6 = []\n",
    "for files in os.listdir('./Data/Test_2'):\n",
    "\n",
    "    if files== 'desktop.ini':\n",
    "        pass\n",
    "    else:    \n",
    "        test_img = get_image(f'./Data/Test_2/{files}')\n",
    "        image_batch = np.expand_dims(test_img, axis=0)\n",
    "        image_batch = np.copy(image_batch)\n",
    "        image_batch = resnet50.preprocess_input(image_batch)\n",
    "        feature_input = resnet_del_model.predict(image_batch)\n",
    "        predictions = model_del.predict(feature_input)\n",
    "        pos = np.argmax(predictions)\n",
    "        Test_prediction6.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "59cc85dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.15357142857142858\n"
     ]
    }
   ],
   "source": [
    "test_acc = (Test_prediction6 == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02421a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
